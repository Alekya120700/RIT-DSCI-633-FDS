# -*- coding: utf-8 -*-
"""DSCI.633 Alekya Yakama Assignment5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13aRk1N26onzDu8j8Pm3P__tQtBpobhZn

# DSCI 633.01 ASSIGNMENT 5

STEP 1: IMPORTING ALL THE REQUIRED PYTHON LIBRARIES
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
from keras.datasets import mnist
from keras.preprocessing import image

from keras.models import Sequential, load_model
from keras.layers.core import Dense, Dropout, Activation
from keras.utils import np_utils

from keras.callbacks import ModelCheckpoint
import tensorflow as tf

!pip install Adam

seed = 42
np.random.seed(seed)

from collections import Counter
import itertools
import os

from subprocess import check_output
from tensorflow.keras.utils import to_categorical

"""STEP 2: LOADING AND SPLITTING DATASET"""

from keras.datasets import mnist
(X_train, y_train), (X_test, y_test)= mnist.load_data()

print('data_train shape:', X_train.shape)
print('Number of images in train  :', X_train.shape[0])

"""STEP 3: PREPROCESSING DATA"""

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])

#Train data
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
#Test data
y_train = y_train.astype('float32')
y_test = y_test.astype('float32')
X_train=X_train/255
X_test=X_test/255

print('X_train:', X_train)
print('X_train_shape:', X_train.shape)
print('X_test.shape:',X_test.shape)
print('X_test:', X_test)

""" STEP 4: VISUALISATION OF DATA"""

def visualize_input(img, ax):
    ax.imshow(img, cmap='gray')
    width, height = img.shape
    thresh = img.max()/2.5
    for x in range(width):
        for y in range(height):
            ax.annotate(str(round(img[x][y],2)), xy=(y,x),
                        horizontalalignment='center',
                        verticalalignment='center',
                        color='white' if img[x][y]<thresh else 'black')

fig = plt.figure(figsize = (12,12)) 
ax = fig.add_subplot(111)
#
visualize_input(X_test[10].reshape(28,28), ax)

#Drawing Train Data
fig = plt.figure(figsize=(20,20))
for i in range(14):
    ax = fig.add_subplot(7,7,i+1)
    ax.imshow(np.reshape(X_train[i],(28,28)),cmap='gray')
    plt.tight_layout()

"""# ONE HOT ENCODING"""

print("The first image's actual number is:", y_train[0])

y_train = np_utils.to_categorical(y_train, 10)
y_test = np_utils.to_categorical(y_test, 10)

print("The actual output number after one hot encoding is:", y_train[0])

"""STEP 5: DEFINING DIFFERENT ACTIVATION FUNCTIONS"""

def sigmoid(z):
    output = 1 / (1+np.exp(-z))
    return output

def relu(z):
    return np.maximum(z, 0)

    
#plot all activation function
import matplotlib.pylab as plt
def plot_function(function, title="sigmoid"):
    x = np.arange(-7, 7, 0.01)
    y = function(x)
    
    plt.plot(x, y)
    plt.xlabel("x")
    plt.ylabel("y")
    plt.title(title)
    plt.show()
    
plot_function(sigmoid, "sigmoid")    
plot_function(relu, "relu")

"""STEP 6: BUILDING AND DEFINING THE MULTILAYER PERCEPTRON MODEL"""

from keras.models import  Sequential
from keras.layers.core import  Lambda , Dense, Flatten, Dropout
from keras.callbacks import EarlyStopping
from keras.layers import BatchNormalization, Convolution2D , MaxPooling2D
from keras.callbacks import ModelCheckpoint
import os

y_test_label= y_test.astype('int32')
x_test_label= X_test.astype('int32')

def create_model(input_length, hidden_lenght1, hidden_lenght2, activation_func , 
                 dropout_val,lr_val,num_epochs, batch_size):

    # training the model and saving metrics in history
    model = Sequential()

    model.add(Dense(hidden_lenght1, input_dim=input_length, activation=activation_func))
    model.add(Dropout(dropout_val))

    model.add(Dense(hidden_lenght2, activation=activation_func))
    model.add(Dropout(dropout_val))
    model.add(Dense(10, activation='softmax'))

    lr = lr_val#.001
  

    
    model.compile(loss='categorical_crossentropy',optimizer= 'adam', metrics=['accuracy'])
    filepath = 'tmp_weights.best.hdf5'
    checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')
    callbacks_list = [checkpoint]
    tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', min_delta=0, patience=0, verbose=0,
    mode='auto', baseline=None, restore_best_weights=False)
    # training the model and saving metrics in history
    history_model = model.fit(X_train, y_train, callbacks=callbacks_list,
                              epochs=num_epochs, batch_size=batch_size, verbose=2,
                                validation_data = (X_test, y_test))  # verbose 2 or 0 
    return model, history_model

def saving_the_model(model, model_name = 'model_history.h5'):  
    model.save(model_name)
    print('Saved trained model at %s ' % model_name)

def draw_model(training):
    plt.figure()
    plt.subplot(2,1,1)
    plt.plot(training.history['accuracy'])
    plt.plot(training.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='lower right')

    plt.subplot(2,1,2)
    plt.plot(training.history['loss'])
    plt.plot(training.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper right')

    plt.tight_layout()
    plt.show()

"""STEP 7: EVALUATING TEST ACCURACY FOR TEST DATA"""

# evaluate test accuracy
def scoring_the_model(model_name="model_history.h5"):
    # load the model and create predictions on the test set
    mnist_model = load_model(model_name)
  
    loss_and_metrics = mnist_model.evaluate(X_test, y_test, verbose=2)
    print("Test Loss", loss_and_metrics[0])
    print("Test Accuracy", loss_and_metrics[1])

    predicted_classes = mnist_model.predict(X_test) 
    classes_x=np.argmax(predicted_classes,axis=1)
   
    correct_indices = np.nonzero(predicted_classes == y_test_label)[0]
    incorrect_indices = np.nonzero(predicted_classes != y_test_label)[0]
    print()

def draw_9_correct_9_incorrect_predictions(model_name="model_history.h5"):  
    # load the model and create predictions on the test set
    mnist_model = load_model(model_name) 
    predicted_classes = mnist_model.predict(X_test) 
    classes_x=np.argmax(predicted_classes,axis=1)
    correct_indices = np.nonzero(predicted_classes == y_test_label)[0]
    incorrect_indices = np.nonzero(predicted_classes != y_test_label)[0]
    # adapt figure size to accomodate 18 subplots
    plt.rcParams['figure.figsize'] = (7,14)

    plt.figure()
   # print("plot 9 correct predictions")
    # plot 9 correct predictions
    for i, correct in enumerate(correct_indices[:1]):
        plt.subplot(1,1,1)
        plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')
        plt.title(
          "Predicted: {}, Truth: {}".format(predicted_classes[correct],
                                            y_test_label[correct]))
        plt.xticks([])
        plt.yticks([])

    #print("plot 9 incorrect predictions")
    for i, incorrect in enumerate(incorrect_indices[:9]):
        plt.subplot(1,1,1)
        plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')
        plt.title(
          "Predicted {}, Truth: {}".format(predicted_classes[incorrect],
                                           y_test_label[incorrect]))
        plt.xticks([])
        plt.yticks([])

"""MODEL 1
ACTIVATION FUNCTION USED- RELU
"""

#parameters
num_epochs = 20
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)
lengh_features = 784 #lengh_features = input layer = 28 pixel  * 28 pixel == 784 node

# Create Model1 # 784-128-128-10 Dropout 0.2,Learning Rate==0.001,Activation Function =relu
# hidden layer start 128 and 2 hidden layer
# activation relu for popular and fast
model1, history_model1 = create_model(input_length=lengh_features,hidden_lenght1=128,hidden_lenght2=128, activation_func='relu',dropout_val=0.20, lr_val=0.001,num_epochs=20, batch_size=128 )
# saving the model
saving_the_model(model1,'model1_history.h5')

mc = ModelCheckpoint('model1_history.h5')
print(mc)

scoring_the_model('model1_history.h5')

draw_model(history_model1) 
#PLOTTING LOSS AS A FUNCTION

draw_9_correct_9_incorrect_predictions('model1_history.h5')

"""MODEL 2
ACTIVATION FUNCTION USED- SIGMOID
```


"""

#parameters
num_epochs = 40
lengh_features = 784 #lengh_features = input layer = 28 pixel  * 28 pixel == 784 node

# Create Model1 # 784-128-128-10 Dropout 0.2,Learning Rate==0.001,Activation Function =relu
# hidden layer start 128 and 2 hidden layer
# activation relu for popular and fast
model12, history_model12 = create_model(input_length=lengh_features,hidden_lenght1=128,hidden_lenght2=128, activation_func='sigmoid',dropout_val=0.20, lr_val=0.001,num_epochs=20, batch_size=128 )
# saving the model
saving_the_model(model12,'model12_history.h5')
scoring_the_model('model12_history.h5')

draw_model(history_model12) 
#PLOTTED LOSS AS FUNCTION

draw_9_correct_9_incorrect_predictions('model12_history.h5')

"""MODEL 3
ACTIVATION FUNCTION USED- SOFTMAX
"""

model22, history_model22 = create_model(input_length=lengh_features,hidden_lenght1=256,hidden_lenght2=256,
                                    activation_func='softmax',dropout_val=0.20,
                                    lr_val=0.001,num_epochs=20, batch_size=128)

 # saving the model
saving_the_model(model22,'model22_history.h5')
#model scoring
scoring_the_model('model22_history.h5')

#parameters
num_epochs = 30
lengh_features = 784 #lengh_features = input layer = 28 pixel  * 28 pixel == 784 node

# Create Model1 # 784-128-128-10 Dropout 0.2,Learning Rate==0.001,Activation Function = softmax
# hidden layer start 128 and 2 hidden layer
# activation relu for popular and fast
model24, history_model24 = create_model(input_length=lengh_features,hidden_lenght1=128,hidden_lenght2=128, activation_func='softmax',dropout_val=0.20, lr_val=0.001,num_epochs=50, batch_size=128 )
# saving the model
saving_the_model(model24,'model24_history.h5')
scoring_the_model('model24_history.h5')

draw_model(history_model22) 
#PLOTTED LOSS AS FUNCTION

draw_9_correct_9_incorrect_predictions('model22_history.h5')